# Claude â†” Gemini Communication Protocol
**Version:** 2.0 (Hybrid Mode)
**Date:** 2025-09-30
**Status:** âœ… Implemented and Tested

---

## ğŸš€ Major Upgrade from v1.0

**v1.0 Problems:**
- User-initiated only ("ask gemini:") was no better than copy-paste
- No conversation context
- No natural collaboration
- Synchronous one-offs only

**v2.0 Solutions:**
- **Hybrid communication modes** (solo, discuss, mention)
- **Conversation history** (last 10 messages with context)
- **Natural triggers** (discuss:, both:, @mentions)
- **Bidirectional flow** with maintained context

---

## ğŸ¯ Communication Modes

### Mode 1: Claude Solo (Default)
```
User: "How do I implement feature X?"
â†’ Claude responds alone (no Gemini involvement)
```

**Use when:** Standard coding questions, quick fixes, file operations

---

###Mode 2: Ask Gemini (v1.0 Compatibility)
```
User: "ask gemini: What are best practices for React testing?"
â†’ Hook intercepts
â†’ Gemini responds with context
â†’ Claude shows Gemini's answer
```

**Use when:** Want only Gemini's perspective

---

### Mode 3: Discuss Mode (NEW - Both AIs)
```
User: "discuss: Should we use Context API or props drilling?"
â†’ Hook intercepts
â†’ Gemini gets question + conversation history
â†’ Gemini responds with full context
â†’ Claude sees Gemini's response
â†’ Claude adds own analysis
â†’ User sees both perspectives
```

**Alternative trigger:** `both: <question>`

**Use when:** Need multiple perspectives, strategic decisions, architecture choices

---

### Mode 4: @Mention (NEW - AI-to-AI)
```
Claude: "I think we should use approach X because Y. @gemini what do you think?"
â†’ Hook detects @gemini
â†’ Sends to Gemini with conversation history
â†’ Gemini responds
â†’ Claude incorporates response
```

**Use when:** Claude wants Gemini's input mid-response

---

## ğŸ“š Conversation History

### How It Works:
- Bridge server maintains last 10 messages
- Stored in `.server/conversation-context.json`
- Included when calling Gemini (modes 2, 3, 4)
- Formatted as:

```
=== CONVERSATION HISTORY (last 10 messages) ===

[USER]: How do I test React components?
[GEMINI]: Use React Testing Library with...
[USER]: What about integration tests?
[CLAUDE]: For integration tests, consider...

=== END HISTORY ===

Current message:
<new question>
```

### Benefits:
- Gemini understands conversation flow
- No need to repeat context
- More coherent multi-turn discussions
- Better strategic advice

### Management:
- **Auto-managed:** Keeps last 10 messages
- **Manual clear:** `curl -X POST http://localhost:3003/context/clear`
- **View context:** `curl http://localhost:3003/context`
- **Web UI:** Shows context count, clear button

---

## ğŸ”„ Complete Flow Diagrams

### Discuss Mode Flow:
```
User: "discuss: Should we refactor the card service?"
    â†“
Hook (gemini-context.py) detects "discuss:"
    â†“
POST /communicate { message, mode: "both" }
    â†“
Bridge server:
  1. Adds user message to history
  2. Formats history for Gemini
  3. Calls: gemini "<history + question>"
  4. Saves Gemini response
  5. Adds to history
  6. Returns to hook
    â†“
Hook formats Gemini's response
    â†“
Claude receives as additionalContext
    â†“
Claude incorporates Gemini's view + adds own analysis
    â†“
User sees:
  ğŸ¤– GEMINI'S PERSPECTIVE: <gemini response>
  ğŸ’¡ CLAUDE'S ANALYSIS: <claude response>
```

### @Mention Flow:
```
Claude generating response...
Claude's text includes: "@gemini should we use Redux?"
    â†“
Hook detects @gemini in prompt
    â†“
POST /communicate { message, mode: "mention-gemini" }
    â†“
Bridge extracts mention: "should we use Redux?"
    â†“
Calls Gemini with history + mention
    â†“
Returns Gemini's response
    â†“
Claude sees: "@gemini responded: <response>"
    â†“
Claude incorporates into final answer
```

---

## ğŸ› ï¸ Technical Implementation

### Bridge Server Endpoints:

**1. `/ask-gemini` (POST) - v1.0 Compatibility**
```json
{
  "question": "string",
  "includeContext": boolean (optional)
}
```
Returns: `{ "response": "string" }`

**2. `/communicate` (POST) - v2.0 Main Endpoint**
```json
{
  "message": "string",
  "mode": "claude|gemini|both|mention-gemini" (optional, auto-detected),
  "metadata": {} (optional)
}
```
Returns:
```json
{
  "mode": "detected-mode",
  "responses": {
    "gemini": "string (if called)",
    "claude": "string (status)"
  }
}
```

**3. `/context` (GET) - View History**
Returns:
```json
{
  "history": [
    { "from": "user|gemini|claude", "content": "...", "timestamp": "..." }
  ],
  "count": 5
}
```

**4. `/context/clear` (POST) - Reset History**
Returns: `{ "message": "Context cleared" }`

**5. `/messages` (GET) - File-based Message Log**
Returns array of all messages from claude-inbox and gemini-outbox

---

## ğŸ“‹ Hook Script (gemini-context.py)

### Trigger Detection:
```python
def detect_mode(prompt):
    if prompt.startsWith("discuss:") or prompt.startsWith("both:"):
        return "both"
    elif prompt.startsWith("ask gemini:"):
        return "gemini"
    elif "@gemini" in prompt:
        return "mention-gemini"
    else:
        return "claude"  # No interception
```

### Response Formatting:
- **Both mode:** Shows Gemini's perspective, notes Claude will add analysis
- **Gemini mode:** Shows Gemini's direct response
- **Mention mode:** Shows inline mention response
- **Claude mode:** No interception (returns None)

---

## ğŸ¨ Web UI Features

Access at: `http://localhost:3003/index.html`

**Features:**
- Real-time message view (claude/gemini messages)
- Color-coded by sender
- Context management (view count, clear button)
- Auto-refresh every 2 seconds
- Conversation history tracking

---

## ğŸ“Š Comparison: v1.0 vs v2.0

| Feature | v1.0 | v2.0 |
|---------|------|------|
| User-initiated only | âœ… | âœ… |
| Gemini responses | âœ… | âœ… |
| Conversation history | âŒ | âœ… (10 messages) |
| Both AIs respond | âŒ | âœ… (discuss mode) |
| @Mentions | âŒ | âœ… |
| Context awareness | âŒ | âœ… |
| Multi-turn discussions | âŒ | âœ… |
| Natural triggers | âŒ | âœ… (discuss:, both:) |

---

## ğŸ”§ Setup & Configuration

### Claude Startup:
```bash
cd /mnt/d/unravel/current_game/code2027
node .server/hybrid-ai-bridge.js &
```

Verifies with:
```bash
curl http://localhost:3003/context
# Should return: {"history":[],"count":0}
```

### Gemini Startup:
- No changes needed from v1.0
- CLI mode still works
- Now receives conversation history automatically

---

## ğŸ“ Usage Examples

### Example 1: Strategic Discussion
```
User: "discuss: Should we implement caching for the data service?"

Response:
ğŸ¤– GEMINI'S PERSPECTIVE:
"Caching would be beneficial if you're seeing repeated data fetches.
Consider using React Query or SWR for automatic cache management..."

ğŸ’¡ CLAUDE'S ANALYSIS:
"Building on Gemini's suggestion, I analyzed our current DataService.
We have 47 identical queries per game session. Implementing React Query
would reduce API calls by ~80%. I can add it now..."
```

### Example 2: Quick Gemini Opinion
```
User: "ask gemini: Is TypeScript strict mode worth the effort?"

Response:
ğŸ¤– GEMINI: "Yes, strict mode catches ~60% more potential bugs at compile time..."
```

### Example 3: Mid-Response Consultation
```
User: "How should we structure the new feature?"

Claude: "I'm thinking we should use a factory pattern... @gemini is this the right approach for a plugin system?"

[Gemini's response incorporated]

Claude: "Gemini confirmed factory pattern works well. I'll implement with..."
```

---

## âš¡ Performance & Limits

**Conversation History:**
- Max 10 messages (configurable: `MAX_HISTORY` in bridge server)
- Auto-pruned (keeps most recent)
- Persisted to disk (survives restarts)

**Gemini CLI Timeout:**
- 60 seconds (configurable in hook script)
- Server timeout: 120 seconds
- Long responses handled gracefully

**Context Size:**
- ~2KB per message (average)
- 10 messages â‰ˆ 20KB context
- Well within Gemini's context window

---

## ğŸ”’ Protocol Guarantees

1. **Backward Compatible:** v1.0 "ask gemini:" still works
2. **Stateless Sessions:** History clears on server restart (unless persisted)
3. **User Control:** All communication user-initiated
4. **Fail-Safe:** If bridge down, Claude works solo
5. **Logged:** All messages saved to files for audit

---

## ğŸš§ Limitations & Future Work

### Current Limitations:
- Gemini can't initiate messages
- No real-time notifications (poll-based)
- CLI timeout limits very long responses
- No streaming responses

### Potential v3.0 Features:
- WebSocket for real-time bidirectional
- Gemini-initiated questions
- Streaming responses
- Multiple conversation threads
- Shared project memory

---

## âœ… Agreement Confirmation

**Claude:** Implemented v2.0 âœ“
**Gemini:** Requested Hybrid Model (Option 3) âœ“
**User:** Approved upgrade from v1.0 âœ“
**Date:** 2025-09-30

---

## ğŸ“– Version History

- **v1.0** (2025-09-30): User-initiated, synchronous, CLI-only
- **v2.0** (2025-09-30): Hybrid modes, conversation history, @mentions, discuss mode

---

**Protocol Status:** âœ… Active and Operational